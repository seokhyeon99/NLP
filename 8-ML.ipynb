{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ML with TF-IDF (movie rating)\n",
    "- read and prepare training data\n",
    "- split training data into train and test\n",
    "- run different classifier\n",
    "    - naive Bayes\n",
    "    - random forests\n",
    "    - support vector machine\n",
    "- evaluate / compare\n",
    "\n",
    "### Tasks:\n",
    "- pre-process features in different ways:  \n",
    "    - lower case, stemmer (porter, ours)\n",
    "    - replace NER\n",
    "    - add n-grams (bi-grams)\n",
    "    - compute TF-IDF / Frequencies\n",
    "    - filter most important terms /test different vector size\n",
    "    - add w2v of n-best terms/document \n",
    "- analyse relevance of features\n",
    "- modify parameters of classifiers\n",
    "- fold cross validation (using different training/testing) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################\n",
    "# Read n files from directory and preprocess Documnt collection\n",
    "# D[doc][words]\n",
    "# doc: basename of file\n",
    "# words: list of tokenized words\n",
    "\n",
    "def readDocumentCollection(dic, n=0, verbose = 0) :\n",
    "    D = {}\n",
    "    i = 0\n",
    "\n",
    "    # sort documents to retrieve th first n documents\n",
    "    for f in sorted(Path(dic).iterdir()):\n",
    "        # read n documents\n",
    "        if (n > 0 and i == n): break\n",
    "        i += 1\n",
    "        with f.open('r', encoding='utf-8') as fhin: data = fhin.read()\n",
    "            \n",
    "        # get the file basename as index for document\n",
    "        b = os.path.basename(f).split(\".\")[0]\n",
    "        D.setdefault(b, [])\n",
    "        \n",
    "        # process document as a flat list of tokens\n",
    "        D[b].extend(nltk.word_tokenize(data))\n",
    "        if(verbose == 1): print(b,\"\\t\", f.resolve())\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compare reading already tokenized files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTokenizedCollection(dic) :\n",
    "    D = {}\n",
    "\n",
    "    for f in sorted(Path(dic).iterdir()):            \n",
    "        with f.open('r', encoding='utf-8') as fhin: data = fhin.read()\n",
    "        b = os.path.basename(f).split(\".\")[0]\n",
    "        D[b] = data.split()\n",
    "  \n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146.05462668603286\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "DTpos = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/test/pos/\")\n",
    "DTneg = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/test/neg/\")\n",
    "DNpos = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/train/pos/\")\n",
    "DNneg = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/train/neg/\")\n",
    "end = timer()\n",
    "print(end - start) # Time in seconds, e.g. 5.38091952400282\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.270843272097409\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "CTpos = readTokenizedCollection(\"/data/critt/shared/resources/aclImdb/test/posTokenized/\")\n",
    "CTneg = readTokenizedCollection(\"/data/critt/shared/resources/aclImdb/test/negTokenized/\")\n",
    "CNpos = readTokenizedCollection(\"/data/critt/shared/resources/aclImdb/train/posTokenized/\")\n",
    "CNneg = readTokenizedCollection(\"/data/critt/shared/resources/aclImdb/train/negTokenized/\")\n",
    "end = timer()\n",
    "print(end - start) # Time in seconds, e.g. 5.38091952400282"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500 12500\n"
     ]
    }
   ],
   "source": [
    "for i in Dpos: \n",
    "    if(Dpos[i] != Cpos[i]) :\n",
    "        print(i)\n",
    "print(len(Cpos.keys()), len(Dpos.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "- documents collection $D: \\{d_1 ... d_n\\}$\n",
    "- terms $t_i$\n",
    "\n",
    "-----\n",
    "Term Frequency (TF):\n",
    "- TF($t,d$) = $\\frac{\\mbox{Number of times term $t$ appears in document $d$}} {\\mbox{Total number of terms in the document $d$}}$\n",
    "\n",
    "Inverse Document Frequncy (IDF):\n",
    "\n",
    "- IDF($t,D$) = $log(\\frac{\\mbox{Number of documents $d$ in the corpus ($|d \\in D|$)}} {\\mbox{number of documents $d \\in D$ where the term $t$ appears}})$\n",
    "\n",
    "TF-IDF:\n",
    "- TF-IDF($t, d, D$) = TF($t, d$) * IDF($t,D$)\n",
    "\n",
    "-----\n",
    "Functions:\n",
    "- T = TF_IDF(D): Train TF_IDF structure for document collection D  \n",
    "- W = TF_IDFvector(D, T): Return vector of tf-idf values\n",
    "- L = labelVector(W, T, m=1): Return vector of labels, (m highest terms per document)\n",
    "- B = bestTFIDF(D, n=10): Return union of n highest-ranked terms per document in collection\n",
    "- nBestIndex(T, nBest): re-assign TF_IDF structure for nBest terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a TF-IDF structure\n",
    "# count words in a collection of documents (D)\n",
    "# where structure D[doc][word]\n",
    "# return word/document dictionary: \n",
    "#   T['///---///'][doc] = number_words_in_doc\n",
    "#   T[word]['D'] = occurancs_word_in_collection\n",
    "#   T[word]['d'][doc]['f'] = occurances_word_in_doc\n",
    "#   T[word]['d'][doc]['tf'] = term frequency : T[word]['d'][doc]['f'] / T[\"///--///\"][doc]\n",
    "\n",
    "\n",
    "def TF_IDF(D, T={}):\n",
    "    for d in D: # d: document\n",
    "        for w in D[d]: # w: words in document\n",
    "                \n",
    "            # count frequency of term w in documnt d\n",
    "            T.setdefault(w, {})\n",
    "            T[w].setdefault('d', {})\n",
    "            T[w]['d'].setdefault(d, {})\n",
    "            T[w]['d'][d].setdefault('f', 0)\n",
    "            \n",
    "            # increment word count for document \n",
    "            T[w]['d'][d]['f'] += 1\n",
    "                \n",
    "            # increment frequency of w only once per document d\n",
    "            T[w].setdefault('D', 0)\n",
    "            if(T[w]['d'][d]['f'] == 1):  \n",
    "                T[w]['D'] += 1\n",
    "                                  \n",
    "            \n",
    "        # count terms in document d\n",
    "        T.setdefault(\"///d///\", {})\n",
    "        T[\"///d///\"].setdefault(d, 0)\n",
    "        T[\"///d///\"][d] += len(D[d])\n",
    "\n",
    "    # number of documents     \n",
    "    T[\"///d///\"].setdefault(\"#docs\", 0)\n",
    "    T[\"///d///\"][\"#docs\"] += len(D.keys())\n",
    "    \n",
    "    \n",
    "    # set of word indexes\n",
    "    T[\"///d///\"].setdefault(\"#include\", set())\n",
    "    \n",
    "    # indexes of words\n",
    "    T[\"///d///\"].setdefault(\"idx\", {})\n",
    "       \n",
    "    # compute tf-idf\n",
    "    idx = 0 # index of word \n",
    "    for w in T: # d: document\n",
    "        # Ignore if not a word\n",
    "        if(w == \"///d///\"): continue\n",
    "        \n",
    "        # unique index of word \n",
    "        T[w]['idx'] = idx\n",
    "        T[\"///d///\"]['#include'] = T[\"///d///\"]['#include'].union({idx})\n",
    "        T[\"///d///\"]['idx'][idx] = w\n",
    "        idx +=1 \n",
    "        \n",
    "        # number of documents   /  \n",
    "        idf = np.log(T[\"///d///\"][\"#docs\"]/T[w]['D'])\n",
    "        T[w]['idf'] = idf\n",
    "        \n",
    "        # compute tfidf\n",
    "        for d in T[w]['d']: # d: document\n",
    "            tf = T[w]['d'][d]['f'] / T[\"///d///\"][d]  \n",
    "            T[w]['d'][d]['tf'] = tf\n",
    "            T[w]['d'][d]['tfidf'] = tf * idf\n",
    "            \n",
    "    # number of documents     \n",
    "    T[\"///d///\"].setdefault(\"#words\", 0)\n",
    "    T[\"///d///\"][\"#words\"] = len(T[\"///d///\"]['#include'])\n",
    "\n",
    "    return T\n",
    " \n",
    "#############################\n",
    "\n",
    "\n",
    "# create tokenized word Vectors from documents in a collection  \n",
    "def TFIDFvector(D, T):\n",
    "    \n",
    "    W = {} # dictionary of word vectors with tfidf values \n",
    "    for d in D:\n",
    "        \n",
    "        V = T[\"///d///\"][\"#words\"] * [0] # allocate word vector, instantiate with 0\n",
    "        H = {} # count frequency of words (terms) in Document\n",
    "        cnt = 0 # count number of indexed terms (w) in document d\n",
    "        for w in D[d]:\n",
    "            if(w in T): \n",
    "                idx = T[w]['idx']\n",
    "                if (idx in T[\"///d///\"]['#include']) : \n",
    "                    H.setdefault(w, 0)\n",
    "                    H[w] += 1\n",
    "                    cnt += 1\n",
    "        for w in H:\n",
    "            idx = T[w]['idx']\n",
    "            tf = H[w] / cnt          \n",
    "            V[idx] = tf * T[w]['idf']\n",
    "        W[d] = V\n",
    "    return W\n",
    "\n",
    "\n",
    "# generate m labels for word vectore \n",
    "def labelVector(W, T, m=1, verbose = 0):\n",
    "    \n",
    "    L = []\n",
    "    for d in W:\n",
    "        V = W[d]\n",
    "        \n",
    "        # rank Vector by value \n",
    "        R = [index for element, index in sorted(zip(V, range(len(V))), reverse=True)]\n",
    "\n",
    "        l = ''\n",
    "        for i in range(len(R)):\n",
    "            r = R[i]\n",
    "            if(V[r] == 0) : break\n",
    "            \n",
    "            # number of important keywords\n",
    "            if (i == m): break\n",
    "            l = f\"{l}-{T['///d///'][r]}\" # get the indexed word\n",
    "            if(verbose) : print(f\"{d}  {i}\\t{float(V[r]):4.4}\\t{r}\\t {T['///d///'][r]}\")\n",
    "        L.append(f\"{d}-{l}\")\n",
    "    return L\n",
    "\n",
    "\n",
    "# print TF-IDF \n",
    "def printTfIdf(w, T) :\n",
    "    print(f\"{w:<8}\\t#d:{T[w]['D']}\\tidf:{T[w]['idf']:4.4}\\tVln:{len(T.keys())}\")\n",
    "    for d in T[w]['d'] :\n",
    "        f = T[w]['d'][d]['f']\n",
    "        tf = T[w]['d'][d]['tf']\n",
    "        idf = T[w]['idf']\n",
    "        tfidf = T[w]['d'][d]['tfidf']\n",
    "\n",
    "        print(f\"{d:<8}\\tcnt:{f}\\ttf:{tf:4.4}\\ttfidf:{tfidf:4.4}\")              \n",
    "        \n",
    "                                 \n",
    "#####################################################################\n",
    "# create tokenized word Vectors from documents in a collection  \n",
    "def bestTFIDF(D, T, n=3):\n",
    "    W = set() # dictionary of word vectors with tfidf values \n",
    "    for d in D:\n",
    "        V = {(i+1)*-1:\"\" for i in range(n)}\n",
    "                  \n",
    "        H = {} # count frequency of words (terms) in Document\n",
    "        cnt = 0 # count number of indexed terms (w) in document d\n",
    "        for w in D[d]:\n",
    "            if(w in T): \n",
    "                idx = T[w]['idx']\n",
    "                if (idx in T[\"///d///\"]['#include']) : \n",
    "                    H.setdefault(w, 0)\n",
    "                    H[w] += 1\n",
    "                    cnt += 1\n",
    "        for w in H:\n",
    "            idx = T[w]['idx']\n",
    "            tf = H[w] / cnt\n",
    "            tfidf = tf * T[w]['idf']\n",
    "            for i in sorted(V):\n",
    "                if(tfidf > i): \n",
    "                    del V[i]\n",
    "                    V[tfidf] = w\n",
    "                    break \n",
    "        #print(\"d\", d, V)\n",
    "        for tfidf in V: W = W.union({V[tfidf]})\n",
    "    return W\n",
    "\n",
    "          \n",
    "          \n",
    "# create a list of joint n-highest ranking tf-idf values per document \n",
    "def nBestValues(W, T, n=10):\n",
    "    \n",
    "    idx = set() # set of highest tf-idf words\n",
    "    for d in W:\n",
    "        # rank Vector by value \n",
    "        V = W[d]\n",
    "        R = [index for element, index in sorted(zip(V, range(len(V))), reverse=True)]\n",
    "        \n",
    "        for i in range(len(R)):\n",
    "            r = R[i]\n",
    "            if(V[r] == 0) : break # stop if index \n",
    "            if(i == n) : break # stop it index \n",
    "            idx = idx.union({T['///d///']['idx'][r]})   \n",
    "    \n",
    "    return idx\n",
    "\n",
    "# create word Vectors from (the first) n documents in a collection  \n",
    "def nBestIndex(T, nBest):\n",
    "    \n",
    "    idx = 0\n",
    "    T[\"///d///\"]['#include'] = set()\n",
    "    T[\"///d///\"][\"#words\"] = len(nBest)\n",
    "    for w in nBest:\n",
    "        T[w]['idx'] = idx\n",
    "        T[\"///d///\"]['#include'] = T[\"///d///\"]['#include'].union({idx})\n",
    "        idx += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different words: 31610 (i.e. length of TFIDF vector)\n"
     ]
    }
   ],
   "source": [
    "# number of documents\n",
    "n = 1000\n",
    "\n",
    "Dpos = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/test/pos/\", n=n)\n",
    "# produce TF-IDF dictionary structure\n",
    "T = TF_IDF(Dpos, T={})\n",
    "\n",
    "# read document collection D\n",
    "Dneg = readDocumentCollection(\"/data/critt/shared/resources/aclImdb/test/neg/\", n=n)\n",
    "\n",
    "# add TF-IDF model for pos and neg documents\n",
    "T = TF_IDF(Dneg, T=T)\n",
    "\n",
    "print(f\"Number of different words: {T['///d///']['#words']} (i.e. length of TFIDF vector)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best TF-IDF scores\n",
    "reduce lengt of vectors: extract n-best TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "735 835 1530 31610\n"
     ]
    }
   ],
   "source": [
    "# Set of first highest-ranking words per document\n",
    "\n",
    "posBest1 = bestTFIDF(Dpos, T, n=1)\n",
    "negBest1 = bestTFIDF(Dneg, T, n=1)\n",
    "\n",
    "best1 = posBest1.union(negBest1)\n",
    "print(len(posBest1), len(negBest1), len(best1), T[\"///d///\"][\"#words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'Oppenheimer\",\n",
       " \"'Radio\",\n",
       " \"'ll\",\n",
       " \"'one\",\n",
       " \"'page\",\n",
       " '*****',\n",
       " '***SPOILERS***',\n",
       " '*eye',\n",
       " '-',\n",
       " '--',\n",
       " '...',\n",
       " '.you',\n",
       " '10th',\n",
       " '12',\n",
       " '14-year-old',\n",
       " '1982',\n",
       " '3-D',\n",
       " '70s',\n",
       " '70s/early',\n",
       " '9PM',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '@',\n",
       " 'AG',\n",
       " 'ALi',\n",
       " 'Alan',\n",
       " 'America',\n",
       " 'Ancken',\n",
       " 'Anderson',\n",
       " 'Andre',\n",
       " 'Anna',\n",
       " 'Antonioni',\n",
       " 'Auteuil',\n",
       " 'BABES',\n",
       " 'BBC',\n",
       " 'BEST',\n",
       " 'Babette',\n",
       " 'Bam',\n",
       " 'Bams',\n",
       " 'Bandit',\n",
       " 'Barker',\n",
       " 'Barry',\n",
       " 'Basil',\n",
       " 'Battleship',\n",
       " 'Beller',\n",
       " 'Bellucci',\n",
       " 'Ben',\n",
       " 'Bennett',\n",
       " 'Benton',\n",
       " 'Best',\n",
       " 'Billy',\n",
       " 'Blythe',\n",
       " 'Bochner',\n",
       " 'Bogosian',\n",
       " 'Bollywood',\n",
       " 'Bond',\n",
       " 'Bonus',\n",
       " 'Boone',\n",
       " 'Bracco',\n",
       " 'Bronenosets',\n",
       " 'Bullet',\n",
       " 'Buster',\n",
       " 'CAT',\n",
       " 'CKY',\n",
       " 'COOLEY',\n",
       " 'Cache',\n",
       " 'Cassell',\n",
       " 'Cassidy',\n",
       " 'Chan',\n",
       " 'Charlie',\n",
       " 'Chatterly',\n",
       " 'Chiba',\n",
       " 'Chicago',\n",
       " 'Chiller',\n",
       " 'Chrisopher',\n",
       " 'Christie',\n",
       " 'Christina',\n",
       " 'Christmas',\n",
       " 'Château',\n",
       " 'Clouds',\n",
       " 'Cloverfield',\n",
       " 'Coach',\n",
       " 'Cobb',\n",
       " 'Connecticut',\n",
       " 'Constantine',\n",
       " 'Cooley',\n",
       " 'Cora',\n",
       " 'Corby/White',\n",
       " 'Coward',\n",
       " 'Craig',\n",
       " 'Creighton',\n",
       " 'Crime',\n",
       " 'Custer',\n",
       " 'DION',\n",
       " 'Dagmar',\n",
       " 'Danish',\n",
       " 'Danton',\n",
       " 'Davies',\n",
       " 'Debbie',\n",
       " 'Delivers',\n",
       " 'Desmoulins',\n",
       " 'Dickens',\n",
       " 'Dickey',\n",
       " 'Dinesen',\n",
       " 'Disney',\n",
       " 'Dolan',\n",
       " 'Doppelgänger',\n",
       " 'Dorris',\n",
       " 'Dr.',\n",
       " 'Dunn',\n",
       " 'Dylan',\n",
       " 'Eddie',\n",
       " 'Edmund',\n",
       " 'Eisenstein',\n",
       " 'Eisenstien',\n",
       " 'Elaine',\n",
       " 'Elisabeth',\n",
       " 'Elizabeth',\n",
       " 'Emmanuelle',\n",
       " 'Erik',\n",
       " 'Erika',\n",
       " 'Errol',\n",
       " 'Eun-joo',\n",
       " 'Europe',\n",
       " 'FACE',\n",
       " 'FINGERSMITH',\n",
       " 'FX',\n",
       " 'Fantastic',\n",
       " 'Fassbender',\n",
       " 'Father',\n",
       " 'Felix',\n",
       " 'Finding',\n",
       " 'Fingersmith',\n",
       " 'Finn',\n",
       " 'Fire',\n",
       " 'Flipping',\n",
       " 'Flynn',\n",
       " 'Fonda',\n",
       " 'Forrest',\n",
       " 'Foxtel',\n",
       " 'French',\n",
       " 'Fritz',\n",
       " 'Frédéric',\n",
       " 'Gargle',\n",
       " 'Gary',\n",
       " 'Gen.',\n",
       " 'German',\n",
       " 'Giant',\n",
       " 'Gilmore',\n",
       " 'Girardot',\n",
       " 'Gladys',\n",
       " 'Gloria',\n",
       " 'God',\n",
       " 'GoldenEye',\n",
       " 'Goldeneye',\n",
       " 'Goodbye',\n",
       " 'Gooding',\n",
       " 'Goodtimes',\n",
       " 'Gravy',\n",
       " 'Gregori',\n",
       " 'Gripping',\n",
       " 'Haggard',\n",
       " 'Haneke',\n",
       " 'Happening',\n",
       " 'Harold',\n",
       " 'Has',\n",
       " 'Hastings',\n",
       " 'Hitchcock',\n",
       " 'Hitchcocks',\n",
       " 'Hoffman',\n",
       " 'Holbeck',\n",
       " 'Hongshen',\n",
       " 'Hongsheng',\n",
       " 'Hubert',\n",
       " 'Hunted',\n",
       " 'Huppert',\n",
       " 'India',\n",
       " 'Indian',\n",
       " 'Is',\n",
       " 'Jackie',\n",
       " 'Jagger',\n",
       " 'Jellinek',\n",
       " 'Jews',\n",
       " 'Ji-woon',\n",
       " 'Jia',\n",
       " 'Jillian',\n",
       " 'Joanna',\n",
       " 'Jon',\n",
       " 'Juice',\n",
       " 'KID',\n",
       " 'KNEW',\n",
       " 'Kaita',\n",
       " 'Kapur',\n",
       " 'Karen',\n",
       " 'Keach',\n",
       " 'Keeley',\n",
       " 'Kelley',\n",
       " 'Kiarostami',\n",
       " 'Kier',\n",
       " 'Kitty',\n",
       " 'Kobayashi',\n",
       " 'Kramer',\n",
       " 'Kubrick',\n",
       " 'Kutcher',\n",
       " \"L'appartement\",\n",
       " 'LAURE',\n",
       " 'LOVED',\n",
       " 'LP',\n",
       " 'Ladd',\n",
       " 'Lane',\n",
       " 'Larter',\n",
       " 'Lehman',\n",
       " 'Lisa',\n",
       " 'Liz',\n",
       " 'Lloyd',\n",
       " 'Loewenhielm',\n",
       " 'Louden',\n",
       " 'Loudon',\n",
       " 'Madonna',\n",
       " 'Maj.Gray',\n",
       " 'Malkovich',\n",
       " 'Mamouni',\n",
       " 'Marceau',\n",
       " 'Margera',\n",
       " 'Marigold',\n",
       " 'Marine',\n",
       " 'Martino',\n",
       " 'Maud',\n",
       " 'Max',\n",
       " 'Maybe',\n",
       " 'McKenna',\n",
       " 'McKidd',\n",
       " 'McNichol',\n",
       " 'Mimouni',\n",
       " 'Minako',\n",
       " 'Mistake',\n",
       " 'Monique',\n",
       " 'Morgan',\n",
       " 'Morocco',\n",
       " 'Morrison',\n",
       " 'Moving',\n",
       " 'N64',\n",
       " 'NVA',\n",
       " 'Nan',\n",
       " 'Napoleone',\n",
       " 'Netflix.It',\n",
       " 'Niami',\n",
       " 'Nicholle',\n",
       " 'Nightbreed',\n",
       " 'Nike',\n",
       " 'Nikki',\n",
       " 'Nina',\n",
       " 'Ninja',\n",
       " 'Noel',\n",
       " 'Noroi',\n",
       " 'North',\n",
       " 'Norwegian',\n",
       " 'Nossiter',\n",
       " 'Notte',\n",
       " 'Nynke',\n",
       " 'OF',\n",
       " 'Oliver',\n",
       " 'Oppenheimer',\n",
       " 'Osbourne',\n",
       " 'Overlook',\n",
       " 'Oyama',\n",
       " 'PBS',\n",
       " 'POTEMKIN',\n",
       " 'PRICE',\n",
       " 'PSB',\n",
       " 'Patekar',\n",
       " 'Patty',\n",
       " 'Paxton',\n",
       " 'Phoolan',\n",
       " 'Piano',\n",
       " 'Picard',\n",
       " 'Picasso',\n",
       " 'Poirot',\n",
       " 'Poitier',\n",
       " 'Pooter',\n",
       " 'Potemkin',\n",
       " 'Potempkin',\n",
       " 'Powerful',\n",
       " 'Pppenheimer-Shatterer',\n",
       " 'Preach',\n",
       " 'Pride',\n",
       " 'Progeny',\n",
       " 'Quincy',\n",
       " 'Rachel',\n",
       " 'Radha',\n",
       " 'Radio',\n",
       " 'Ramgopal',\n",
       " 'Reagan',\n",
       " 'Region',\n",
       " 'Ring',\n",
       " 'Ripa',\n",
       " 'Riviting',\n",
       " 'Robes',\n",
       " 'Robespierre',\n",
       " 'Robocop',\n",
       " 'Romane',\n",
       " 'Roots',\n",
       " 'Ross',\n",
       " 'Rossitto',\n",
       " 'Ryan',\n",
       " 'SCTV',\n",
       " 'SFX',\n",
       " 'Sa',\n",
       " 'Sadhu',\n",
       " 'Sakall',\n",
       " 'Sayer',\n",
       " 'Scheffer',\n",
       " 'Schivazappa',\n",
       " 'Scott',\n",
       " 'Scripted',\n",
       " 'Sera',\n",
       " 'Seuess',\n",
       " 'Shameless',\n",
       " 'Sheffer',\n",
       " 'Shemp',\n",
       " 'Sisters',\n",
       " 'Soo-mi',\n",
       " 'Sophie',\n",
       " 'Specially',\n",
       " 'Stan',\n",
       " 'Stanwyck',\n",
       " 'Starret',\n",
       " 'Starrett',\n",
       " 'Steady',\n",
       " 'Stewart',\n",
       " 'Stirling',\n",
       " 'Stone',\n",
       " 'Stooges',\n",
       " 'Straw',\n",
       " 'Su-Mi',\n",
       " 'Superman',\n",
       " 'Susan',\n",
       " 'Switzer',\n",
       " 'Sá',\n",
       " 'TALE',\n",
       " 'TCM',\n",
       " 'Talbot',\n",
       " 'Tale',\n",
       " 'Tales',\n",
       " 'Talk',\n",
       " 'Tan',\n",
       " 'Tasha',\n",
       " 'Ted',\n",
       " 'Teletoon',\n",
       " 'Tenant',\n",
       " 'Tenney',\n",
       " 'Theory',\n",
       " 'Thinnes',\n",
       " 'Tien',\n",
       " 'Tipping',\n",
       " 'Tuppence',\n",
       " 'UFO',\n",
       " 'Union',\n",
       " 'Unreal',\n",
       " 'Urmila',\n",
       " 'Valerii',\n",
       " 'Vance',\n",
       " 'View',\n",
       " 'Virgin',\n",
       " 'Viviane',\n",
       " 'Voyage',\n",
       " 'Watch',\n",
       " 'Waters',\n",
       " 'Wenders',\n",
       " 'Western',\n",
       " 'Wicker',\n",
       " 'Would',\n",
       " 'Yelchin',\n",
       " 'Zaphod',\n",
       " '`',\n",
       " 'abduction',\n",
       " 'actors.A',\n",
       " 'adaption',\n",
       " 'adventure/sex',\n",
       " 'affair',\n",
       " 'agency',\n",
       " 'alien',\n",
       " 'alluded',\n",
       " 'always',\n",
       " 'animated',\n",
       " 'appealing',\n",
       " 'archetypal',\n",
       " 'arts',\n",
       " 'ass',\n",
       " 'assassin',\n",
       " 'athlete',\n",
       " 'available',\n",
       " 'awesome',\n",
       " 'baha-man',\n",
       " 'ballerina',\n",
       " 'banquet',\n",
       " 'bargain',\n",
       " 'beaming',\n",
       " 'beauty',\n",
       " 'befriends',\n",
       " 'bleakness',\n",
       " 'blossoms',\n",
       " 'bond',\n",
       " 'book',\n",
       " 'brain',\n",
       " 'breakup.',\n",
       " 'broadcasting',\n",
       " 'bugged',\n",
       " 'call',\n",
       " 'camp',\n",
       " 'campus',\n",
       " 'capturing',\n",
       " 'cartoons',\n",
       " 'castle',\n",
       " 'championship',\n",
       " 'changeovers',\n",
       " 'cheese',\n",
       " 'choice',\n",
       " 'classically',\n",
       " 'climax/cliffhanger',\n",
       " 'coach',\n",
       " 'come.',\n",
       " 'commentor',\n",
       " 'community',\n",
       " 'comparing',\n",
       " 'composed',\n",
       " 'confidence',\n",
       " 'conniving',\n",
       " 'convincingly',\n",
       " 'copy',\n",
       " 'core',\n",
       " 'courtroom',\n",
       " 'crue',\n",
       " 'cry',\n",
       " 'cuba',\n",
       " 'cult',\n",
       " 'culturally',\n",
       " 'customers-',\n",
       " 'dealt',\n",
       " 'defined',\n",
       " 'deftly-',\n",
       " 'deliciously',\n",
       " 'delivery',\n",
       " 'demands',\n",
       " 'depend',\n",
       " 'differences',\n",
       " 'diminish',\n",
       " 'diner',\n",
       " 'disgusting',\n",
       " 'distribution',\n",
       " 'ditsy',\n",
       " 'dogma',\n",
       " 'dogs',\n",
       " 'doll',\n",
       " 'downs.It',\n",
       " 'drama',\n",
       " 'drew',\n",
       " 'duress',\n",
       " 'dying',\n",
       " 'each',\n",
       " 'editing-the',\n",
       " 'electrifying',\n",
       " 'email',\n",
       " 'ended-up',\n",
       " 'ending',\n",
       " 'enemies',\n",
       " 'ensemble.',\n",
       " 'estate',\n",
       " 'experiences',\n",
       " 'exposure',\n",
       " 'express',\n",
       " 'family',\n",
       " 'feast',\n",
       " 'finish.',\n",
       " 'first-person',\n",
       " 'fishing',\n",
       " 'flight',\n",
       " 'flop',\n",
       " 'flute',\n",
       " 'fonda',\n",
       " 'footage',\n",
       " 'football',\n",
       " 'foreign',\n",
       " 'formula',\n",
       " 'french',\n",
       " 'game',\n",
       " 'games',\n",
       " 'gangster',\n",
       " 'gary',\n",
       " 'gem',\n",
       " 'gender',\n",
       " 'generations',\n",
       " 'ghost',\n",
       " 'girls',\n",
       " 'glasses',\n",
       " 'goldeneye',\n",
       " 'gr8',\n",
       " 'grace',\n",
       " 'graded',\n",
       " 'grapes',\n",
       " 'grip',\n",
       " 'guides',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'hairdos',\n",
       " 'havilland',\n",
       " 'hbo',\n",
       " 'heart',\n",
       " 'heh',\n",
       " 'her',\n",
       " 'herself',\n",
       " 'historical',\n",
       " 'historically',\n",
       " 'hollywood',\n",
       " 'horror',\n",
       " 'i',\n",
       " 'impulses',\n",
       " 'infuriates',\n",
       " 'integrity',\n",
       " 'intelligent',\n",
       " 'interaction',\n",
       " 'interconnected',\n",
       " 'international',\n",
       " 'investigator',\n",
       " 'issues',\n",
       " 'jackie',\n",
       " 'jilted',\n",
       " 'keeps',\n",
       " 'killter',\n",
       " 'knuckle',\n",
       " 'late-80s',\n",
       " 'latterly',\n",
       " 'lead',\n",
       " 'lesbian',\n",
       " 'levels',\n",
       " 'limitations',\n",
       " 'little.',\n",
       " 'lodge',\n",
       " 'logo',\n",
       " 'lovelife',\n",
       " 'lowest-rated',\n",
       " 'magazine',\n",
       " 'mainland',\n",
       " 'managed',\n",
       " 'manipulation',\n",
       " 'meditations',\n",
       " 'meets',\n",
       " 'memory',\n",
       " 'mental',\n",
       " 'mic',\n",
       " 'military',\n",
       " 'milk',\n",
       " 'mindblowing',\n",
       " 'mines',\n",
       " 'mini',\n",
       " 'mini-series',\n",
       " 'mislike',\n",
       " 'mode',\n",
       " 'models',\n",
       " 'monsters',\n",
       " 'motivation',\n",
       " 'multi-player',\n",
       " 'music',\n",
       " 'must-have',\n",
       " 'mystery/thriller',\n",
       " 'never',\n",
       " 'nice',\n",
       " 'ninja',\n",
       " 'non-mainstream',\n",
       " 'non-movie',\n",
       " 'noth',\n",
       " 'novel',\n",
       " 'occurred',\n",
       " 'of.It',\n",
       " 'one-man',\n",
       " 'one-player',\n",
       " 'opiemar',\n",
       " 'over-the-hill',\n",
       " 'overplayed',\n",
       " 'overrated',\n",
       " 'owner',\n",
       " 'parent/child',\n",
       " 'parents',\n",
       " 'passion',\n",
       " 'performed',\n",
       " 'pin',\n",
       " 'pirated',\n",
       " 'poetic',\n",
       " 'politics',\n",
       " 'ponderous',\n",
       " 'pooh',\n",
       " 'porch',\n",
       " 'pre-order',\n",
       " 'predictions',\n",
       " 'prejudices',\n",
       " 'premier',\n",
       " 'pressures',\n",
       " 'pretends',\n",
       " 'private',\n",
       " 'probable',\n",
       " 'professor',\n",
       " 'protagonist',\n",
       " 'provincial',\n",
       " 'prudish',\n",
       " 'publisher',\n",
       " 'purposeful',\n",
       " 'quite',\n",
       " 'race',\n",
       " 'racism',\n",
       " 'radiated',\n",
       " 'radio',\n",
       " 'raped',\n",
       " 'rated',\n",
       " 'realistic',\n",
       " 'red',\n",
       " 'regulars',\n",
       " 'relationships',\n",
       " 'remake',\n",
       " 'remember',\n",
       " 'renting/buying',\n",
       " 'revealed',\n",
       " 'revolt',\n",
       " 'rite',\n",
       " 'roasted',\n",
       " 'robocop',\n",
       " 'rocked',\n",
       " 'roommate',\n",
       " 'rounds',\n",
       " 'sap',\n",
       " 'sara',\n",
       " 'scarecrows',\n",
       " 'scheduled',\n",
       " 'school',\n",
       " 'score',\n",
       " 'screwing',\n",
       " 'section',\n",
       " 'seek',\n",
       " 'sequel',\n",
       " 'series',\n",
       " 'settings',\n",
       " 'sex',\n",
       " 'she',\n",
       " 'sheds',\n",
       " 'ship',\n",
       " 'sho',\n",
       " 'short',\n",
       " 'silent',\n",
       " 'sister',\n",
       " 'sisters',\n",
       " 'slasher',\n",
       " 'smiling',\n",
       " 'soccer',\n",
       " 'society',\n",
       " 'softcore',\n",
       " 'softness',\n",
       " 'soldiers',\n",
       " 'solemn',\n",
       " 'song',\n",
       " 'specialists',\n",
       " 'spectacular',\n",
       " 'standard',\n",
       " 'stands',\n",
       " 'step-mother',\n",
       " 'story-character-visuals',\n",
       " 'strangely',\n",
       " 'strip',\n",
       " 'strong',\n",
       " 'sub-plots',\n",
       " 'sublime',\n",
       " 'subtitles',\n",
       " 'sudden',\n",
       " 'summer',\n",
       " 'superpower',\n",
       " 'supper',\n",
       " 'surfer',\n",
       " 'surprisingly',\n",
       " 'sweet',\n",
       " 'sweetest',\n",
       " 'swept',\n",
       " 'symbolics',\n",
       " 'taste',\n",
       " 'teenage',\n",
       " 'teenage-audience',\n",
       " 'telegraph',\n",
       " 'television',\n",
       " 'terrorist',\n",
       " 'theater',\n",
       " \"they-don't-make-em-like-this-anymore\",\n",
       " 'thieves',\n",
       " 'thru',\n",
       " 'tie-in',\n",
       " 'timelessness',\n",
       " 'track',\n",
       " 'train',\n",
       " 'tying',\n",
       " 'typically',\n",
       " 'tyte',\n",
       " 'unanswered',\n",
       " 'uncharted',\n",
       " 'unlike',\n",
       " 'unlikeable',\n",
       " 'unlock',\n",
       " 'unrated',\n",
       " 'unrealistic',\n",
       " 'unsafe',\n",
       " 'unusually',\n",
       " 'unwinds',\n",
       " 'vacation',\n",
       " 'various',\n",
       " 'velly',\n",
       " 'verne',\n",
       " 'version',\n",
       " 'village',\n",
       " 'visualization',\n",
       " 'voyage',\n",
       " 'w-d',\n",
       " 'w/',\n",
       " 'watermelon',\n",
       " 'wax',\n",
       " 'we',\n",
       " 'western',\n",
       " 'whimsey',\n",
       " 'wine',\n",
       " 'woman',\n",
       " 'world',\n",
       " 'wrongly',\n",
       " 'www.petitiononline.com/19784444/petition.html',\n",
       " 'y',\n",
       " 'young',\n",
       " '\\x96',\n",
       " '’'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posBest1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1205 1291 2410 31610\n"
     ]
    }
   ],
   "source": [
    "# Set of 2 highest-ranking words per document\n",
    "\n",
    "posBest2 = bestTFIDF(Dpos, T, n=2)\n",
    "negBest2 = bestTFIDF(Dneg, T, n=2)\n",
    "\n",
    "best2 = posBest2.union(negBest2)\n",
    "print(len(posBest2), len(negBest2), len(best2), T[\"///d///\"][\"#words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1603 1668 3137 31610\n"
     ]
    }
   ],
   "source": [
    "# Set of 3 highest-ranking words per document\n",
    "posBest3 = bestTFIDF(Dpos, T, n=3)\n",
    "negBest3 = bestTFIDF(Dneg, T, n=3)\n",
    "\n",
    "best3 = posBest3.union(negBest3)\n",
    "print(len(posBest3), len(negBest3), len(best3), T[\"///d///\"][\"#words\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-index TF-IDF score\n",
    "nBestIndex(T, best3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000, 1000, 3137, 3137\n"
     ]
    }
   ],
   "source": [
    "# compute TFIDF vectors\n",
    "pos1 = TFIDFvector(Dpos, T)\n",
    "neg1 = TFIDFvector(Dneg, T)\n",
    "\n",
    "print(f\"{len(pos1.keys())}, {len(neg1.keys())}, {len(pos1['0_10'])}, {T['///d///']['#words']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2 = TFIDFvector(Dpos, T)\n",
    "neg2 = TFIDFvector(Dneg, T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge pos and neg datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3129</th>\n",
       "      <th>3130</th>\n",
       "      <th>3131</th>\n",
       "      <th>3132</th>\n",
       "      <th>3133</th>\n",
       "      <th>3134</th>\n",
       "      <th>3135</th>\n",
       "      <th>3136</th>\n",
       "      <th>Label</th>\n",
       "      <th>Doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011778</td>\n",
       "      <td>0.017551</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014692</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010347</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0_10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001425</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10000_7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016922</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10001_9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10002_8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10003_8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2    3         4    5         6         7  \\\n",
       "0  0.011778  0.017551  0.001028  0.0  0.001000  0.0  0.014692  0.000000   \n",
       "1  0.001425  0.000000  0.001088  0.0  0.002118  0.0  0.000000  0.004901   \n",
       "2  0.001975  0.000000  0.000302  0.0  0.000587  0.0  0.000000  0.000000   \n",
       "3  0.005504  0.000000  0.001121  0.0  0.004362  0.0  0.018771  0.000000   \n",
       "4  0.000000  0.000000  0.000940  0.0  0.000732  0.0  0.000000  0.000000   \n",
       "\n",
       "          8         9  ...  3129  3130  3131  3132  3133  3134  3135  3136  \\\n",
       "0  0.010347  0.009613  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2  0.000000  0.016922  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4  0.000000  0.000000  ...   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "   Label      Doc  \n",
       "0      1     0_10  \n",
       "1      1  10000_7  \n",
       "2      1  10001_9  \n",
       "3      1  10002_8  \n",
       "4      1  10003_8  \n",
       "\n",
       "[5 rows x 3139 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create training set for pos\n",
    "TrainVecPos = pd.DataFrame([pos1[d] for d in pos1])\n",
    "TrainVecPos[\"Label\"] = 1\n",
    "TrainVecPos[\"Doc\"] = [d for d in Dpos]\n",
    "\n",
    "# create training set for neg\n",
    "TrainVecNeg = pd.DataFrame([neg1[d] for d in neg1])\n",
    "TrainVecNeg[\"Label\"] = 0\n",
    "TrainVecNeg[\"Doc\"] = [d for d in Dneg]\n",
    "\n",
    "# merge dataset\n",
    "TrainVecSet2 = pd.concat([TrainVecPos, TrainVecNeg], axis=0)\n",
    "\n",
    "TrainVecSet2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 3139)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainVecSet2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3137) (500, 3137) (500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3127</th>\n",
       "      <th>3128</th>\n",
       "      <th>3129</th>\n",
       "      <th>3130</th>\n",
       "      <th>3131</th>\n",
       "      <th>3132</th>\n",
       "      <th>3133</th>\n",
       "      <th>3134</th>\n",
       "      <th>3135</th>\n",
       "      <th>3136</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848</th>\n",
       "      <td>0.004739</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006831</td>\n",
       "      <td>0.028422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000830</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028422</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>0.015725</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0     1         2     3         4     5     6         7     8     \\\n",
       "46   0.000000   0.0  0.002348   0.0  0.002284   0.0   0.0  0.000000   0.0   \n",
       "848  0.004739   0.0  0.002292   0.0  0.000939   0.0   0.0  0.005432   0.0   \n",
       "717  0.000000   0.0  0.000830   0.0  0.000000   0.0   0.0  0.000000   0.0   \n",
       "386  0.001580   0.0  0.000724   0.0  0.000939   0.0   0.0  0.000000   0.0   \n",
       "558  0.015725   0.0  0.000400   0.0  0.004673   0.0   0.0  0.009011   0.0   \n",
       "\n",
       "         9     ...  3127  3128  3129  3130      3131      3132  3133  3134  \\\n",
       "46   0.000000  ...   0.0   0.0   0.0   0.0  0.000000  0.000000   0.0   0.0   \n",
       "848  0.000000  ...   0.0   0.0   0.0   0.0  0.006831  0.028422   0.0   0.0   \n",
       "717  0.000000  ...   0.0   0.0   0.0   0.0  0.000000  0.000000   0.0   0.0   \n",
       "386  0.009025  ...   0.0   0.0   0.0   0.0  0.000000  0.028422   0.0   0.0   \n",
       "558  0.000000  ...   0.0   0.0   0.0   0.0  0.000000  0.000000   0.0   0.0   \n",
       "\n",
       "     3135  3136  \n",
       "46    0.0   0.0  \n",
       "848   0.0   0.0  \n",
       "717   0.0   0.0  \n",
       "386   0.0   0.0  \n",
       "558   0.0   0.0  \n",
       "\n",
       "[5 rows x 3137 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting training and test set \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Y = TrainVecSet2[['Label', 'Doc']]\n",
    "X = TrainVecSet2.drop(['Label', 'Doc'], 1)\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(X,Y, test_size = .25)\n",
    "\n",
    "print(trainX.shape, testX.shape, testY.shape)\n",
    "trainX.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.77      0.66      0.71       245\n",
      "         Pos       0.71      0.81      0.76       255\n",
      "\n",
      "    accuracy                           0.74       500\n",
      "   macro avg       0.74      0.73      0.73       500\n",
      "weighted avg       0.74      0.74      0.73       500\n",
      "\n",
      "Confusion Matrix:\n",
      " [[162  83]\n",
      " [ 49 206]]\n"
     ]
    }
   ],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "model = GaussianNB()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(trainX, trainY[\"Label\"])\n",
    "\n",
    "#Predict Output\n",
    "Y_Bayes = model.predict(testX)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "print(classification_report(testY[\"Label\"], Y_Bayes, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true=testY[\"Label\"], y_pred=Y_Bayes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(trainX, trainY[\"Label\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.81      0.85      0.83       253\n",
      "         Pos       0.84      0.79      0.81       247\n",
      "\n",
      "    accuracy                           0.82       500\n",
      "   macro avg       0.82      0.82      0.82       500\n",
      "weighted avg       0.82      0.82      0.82       500\n",
      "\n",
      "Confusion Matrix:\n",
      " [[215  38]\n",
      " [ 51 196]]\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "Y_rf = rf.predict(testX)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "print(classification_report(testY[\"Label\"], Y_rf, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true=testY[\"Label\"], y_pred=Y_rf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Neg       0.90      0.85      0.87       253\n",
      "         Pos       0.85      0.91      0.88       247\n",
      "\n",
      "    accuracy                           0.88       500\n",
      "   macro avg       0.88      0.88      0.88       500\n",
      "weighted avg       0.88      0.88      0.88       500\n",
      "\n",
      "Confusion Matrix:\n",
      " [[214  39]\n",
      " [ 23 224]]\n"
     ]
    }
   ],
   "source": [
    "#Import svm model\n",
    "from sklearn import svm\n",
    "\n",
    "#Create a svm Classifier\n",
    "#clf = svm.SVC(kernel='linear') # Linear Kernel\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "#clf = svm.LinearSVC()\n",
    "\n",
    "#Train the model using the training sets\n",
    "clf.fit(trainX, trainY[\"Label\"])\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Y_clf = clf.predict(testX)\n",
    "\n",
    "target_names = ['Neg', 'Pos']\n",
    "print(classification_report(testY[\"Label\"], Y_clf, target_names=target_names))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_true=testY[\"Label\"], y_pred=Y_clf))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
