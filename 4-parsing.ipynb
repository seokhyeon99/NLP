{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing \n",
    "- NLTK\n",
    "- SPACY\n",
    "- STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "michelle = \"\"\"Hi! I'm Michelle and I'm 22.\n",
    "I really,really like this guy. He's 27 and everything  I like in a guy. We have so much in common.\n",
    "We met around three and a half months ago. A week after we met, he texted me and we didn't stop talking for a whole month and a half. We talked day and night, sometimes 'til four in the morning.\n",
    "Then, he started ignoring me. When that started to  happen, a red flag went up in my head, so I started ignoring him, too. Except I started missing him.\n",
    "Before I started a new semester, I asked him what was the point of saving my number if he wasn't going to ask me out. (Yes, we haven't gone out on a date yet. We've talked about it, but he doesn't make it happen.)\n",
    "I told him I wasn't going to have enough time for him, and if he really wanted to go out with me, he should make it happen soon rather than later.\n",
    "I just don't understand why he hasn't asked me out yet. He gives me the money excuse, or the \"every time I want to, something else comes up\" excuse.\n",
    "If he wants to see me he should've done so already... right?\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "groucho_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (VP (V shot) (NP (Det an) (N elephant)))\n",
      "    (PP (P in) (NP (Det my) (N pajamas)))))\n",
      "(S\n",
      "  (NP I)\n",
      "  (VP\n",
      "    (V shot)\n",
      "    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
     ]
    }
   ],
   "source": [
    "elephant = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(groucho_grammar)\n",
    "\n",
    "for tree in parser.parse(elephant):\n",
    "    print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Mary/NN)\n",
      "  saw/VBD\n",
      "  (CLAUSE\n",
      "    (NP the/DT cat/NNP)\n",
      "    (VP sit/VB (PP on/IN (NP the/DT mat/NN)))))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|JJ|NN.*>+}          # Chunk sequences of DT, JJ, NN\n",
    "  PP: {<IN><NP>}               # Chunk prepositions followed by NP\n",
    "  VP: {<VB.*><NP|PP|CLAUSE>+$} # Chunk verbs and their arguments\n",
    "  CLAUSE: {<NP><VP>}           # Chunk NP, VP\n",
    "  \"\"\"\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "mary = [(\"Mary\", \"NN\"), (\"saw\", \"VBD\"), (\"the\", \"DT\"), (\"cat\", \"NNP\"),\n",
    "    (\"sit\", \"VB\"), (\"on\", \"IN\"), (\"the\", \"DT\"), (\"mat\", \"NN\")]\n",
    "\n",
    "mary_parse = cp.parse(mary)\n",
    "print(mary_parse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  shot/VBP\n",
      "  (NP an/DT elephant/NN)\n",
      "  in/IN\n",
      "  my/PRP$\n",
      "  (NP pajamas/NN))\n"
     ]
    }
   ],
   "source": [
    "elephant_pos = nltk.pos_tag(elephant)\n",
    "elephant_parse = cp.parse(elephant_pos)\n",
    "print(elephant_parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('words')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def preprocessSent(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "def preprocessText(text):\n",
    "# Segment text into sentences\n",
    "    sent = sent_tokenize(text)\n",
    "# Tokenize each sentences\n",
    "    sent = [nltk.word_tokenize(s) for s in sent]\n",
    "# Part-of-speech tagging each sentences\n",
    "    sent = [nltk.pos_tag(s) for s in sent]\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Hi', 'NN'), ('!', '.')], [('I', 'PRP'), (\"'m\", 'VBP'), ('Michelle', 'NNP'), ('and', 'CC'), ('I', 'PRP'), (\"'m\", 'VBP'), ('22', 'CD'), ('.', '.')], [('I', 'PRP'), ('really', 'RB'), (',', ','), ('really', 'RB'), ('like', 'IN'), ('this', 'DT'), ('guy', 'NN'), ('.', '.')], [('He', 'PRP'), (\"'s\", 'VBZ'), ('27', 'CD'), ('and', 'CC'), ('everything', 'NN'), ('I', 'PRP'), ('like', 'VBP'), ('in', 'IN'), ('a', 'DT'), ('guy', 'NN'), ('.', '.')], [('We', 'PRP'), ('have', 'VBP'), ('so', 'RB'), ('much', 'JJ'), ('in', 'IN'), ('common', 'JJ'), ('.', '.')], [('We', 'PRP'), ('met', 'VBD'), ('around', 'IN'), ('three', 'CD'), ('and', 'CC'), ('a', 'DT'), ('half', 'JJ'), ('months', 'NNS'), ('ago', 'RB'), ('.', '.')], [('A', 'DT'), ('week', 'NN'), ('after', 'IN'), ('we', 'PRP'), ('met', 'VBD'), (',', ','), ('he', 'PRP'), ('texted', 'VBD'), ('me', 'PRP'), ('and', 'CC'), ('we', 'PRP'), ('did', 'VBD'), (\"n't\", 'RB'), ('stop', 'VB'), ('talking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('month', 'NN'), ('and', 'CC'), ('a', 'DT'), ('half', 'NN'), ('.', '.')], [('We', 'PRP'), ('talked', 'VBD'), ('day', 'NN'), ('and', 'CC'), ('night', 'NN'), (',', ','), ('sometimes', 'RB'), (\"'til\", 'CD'), ('four', 'CD'), ('in', 'IN'), ('the', 'DT'), ('morning', 'NN'), ('.', '.')], [('Then', 'RB'), (',', ','), ('he', 'PRP'), ('started', 'VBD'), ('ignoring', 'VBG'), ('me', 'PRP'), ('.', '.')], [('When', 'WRB'), ('that', 'WDT'), ('started', 'VBD'), ('to', 'TO'), ('happen', 'VB'), (',', ','), ('a', 'DT'), ('red', 'JJ'), ('flag', 'NN'), ('went', 'VBD'), ('up', 'RB'), ('in', 'IN'), ('my', 'PRP$'), ('head', 'NN'), (',', ','), ('so', 'IN'), ('I', 'PRP'), ('started', 'VBD'), ('ignoring', 'VBG'), ('him', 'PRP'), (',', ','), ('too', 'RB'), ('.', '.')], [('Except', 'IN'), ('I', 'PRP'), ('started', 'VBD'), ('missing', 'VBG'), ('him', 'PRP'), ('.', '.')], [('Before', 'IN'), ('I', 'PRP'), ('started', 'VBD'), ('a', 'DT'), ('new', 'JJ'), ('semester', 'NN'), (',', ','), ('I', 'PRP'), ('asked', 'VBD'), ('him', 'PRP'), ('what', 'WP'), ('was', 'VBD'), ('the', 'DT'), ('point', 'NN'), ('of', 'IN'), ('saving', 'VBG'), ('my', 'PRP$'), ('number', 'NN'), ('if', 'IN'), ('he', 'PRP'), ('was', 'VBD'), (\"n't\", 'RB'), ('going', 'VBG'), ('to', 'TO'), ('ask', 'VB'), ('me', 'PRP'), ('out', 'RP'), ('.', '.')], [('(', '('), ('Yes', 'UH'), (',', ','), ('we', 'PRP'), ('have', 'VBP'), (\"n't\", 'RB'), ('gone', 'VBN'), ('out', 'RP'), ('on', 'IN'), ('a', 'DT'), ('date', 'NN'), ('yet', 'RB'), ('.', '.')], [('We', 'PRP'), (\"'ve\", 'VBP'), ('talked', 'VBN'), ('about', 'IN'), ('it', 'PRP'), (',', ','), ('but', 'CC'), ('he', 'PRP'), ('does', 'VBZ'), (\"n't\", 'RB'), ('make', 'VB'), ('it', 'PRP'), ('happen', 'VB'), ('.', '.'), (')', ')')], [('I', 'PRP'), ('told', 'VBD'), ('him', 'PRP'), ('I', 'PRP'), ('was', 'VBD'), (\"n't\", 'RB'), ('going', 'VBG'), ('to', 'TO'), ('have', 'VB'), ('enough', 'JJ'), ('time', 'NN'), ('for', 'IN'), ('him', 'PRP'), (',', ','), ('and', 'CC'), ('if', 'IN'), ('he', 'PRP'), ('really', 'RB'), ('wanted', 'VBD'), ('to', 'TO'), ('go', 'VB'), ('out', 'RP'), ('with', 'IN'), ('me', 'PRP'), (',', ','), ('he', 'PRP'), ('should', 'MD'), ('make', 'VB'), ('it', 'PRP'), ('happen', 'VB'), ('soon', 'RB'), ('rather', 'RB'), ('than', 'IN'), ('later', 'RB'), ('.', '.')], [('I', 'PRP'), ('just', 'RB'), ('do', 'VBP'), (\"n't\", 'RB'), ('understand', 'VB'), ('why', 'WRB'), ('he', 'PRP'), ('has', 'VBZ'), (\"n't\", 'RB'), ('asked', 'VBN'), ('me', 'PRP'), ('out', 'RP'), ('yet', 'RB'), ('.', '.')], [('He', 'PRP'), ('gives', 'VBZ'), ('me', 'PRP'), ('the', 'DT'), ('money', 'NN'), ('excuse', 'NN'), (',', ','), ('or', 'CC'), ('the', 'DT'), ('``', '``'), ('every', 'DT'), ('time', 'NN'), ('I', 'PRP'), ('want', 'VBP'), ('to', 'TO'), (',', ','), ('something', 'NN'), ('else', 'RB'), ('comes', 'VBZ'), ('up', 'RP'), (\"''\", \"''\"), ('excuse', 'NN'), ('.', '.')], [('If', 'IN'), ('he', 'PRP'), ('wants', 'VBZ'), ('to', 'TO'), ('see', 'VB'), ('me', 'PRP'), ('he', 'PRP'), ('should', 'MD'), (\"'ve\", 'VBP'), ('done', 'VBN'), ('so', 'RB'), ('already', 'RB'), ('...', ':'), ('right', 'RB'), ('?', '.')]]\n"
     ]
    }
   ],
   "source": [
    "michelle_prep = preprocessText(michelle)\n",
    "print(michelle_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  really/RB\n",
      "  ,/,\n",
      "  really/RB\n",
      "  (PP like/IN (NP this/DT guy/NN))\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "michelle_parse = cp.parse(michelle_prep[2])\n",
    "print(michelle_parse)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token           | Relation | Head            | Children            \n",
      "----------------------------------------------------------------------\n",
      "I               | nsubj    | have            | []                  \n",
      "have            | ROOT     | have            | [I, car, .]         \n",
      "a               | det      | car             | []                  \n",
      "red             | amod     | car             | []                  \n",
      "car             | dobj     | have            | [a, red]            \n",
      ".               | punct    | have            | []                  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"20d1524cb969434fb478af6c31835105-0\" class=\"displacy\" width=\"650\" height=\"317.0\" direction=\"ltr\" style=\"max-width: none; height: 317.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">have</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">red</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"227.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">car.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20d1524cb969434fb478af6c31835105-0-0\" stroke-width=\"2px\" d=\"M70,182.0 C70,122.0 160.0,122.0 160.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20d1524cb969434fb478af6c31835105-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,184.0 L62,172.0 78,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20d1524cb969434fb478af6c31835105-0-1\" stroke-width=\"2px\" d=\"M310,182.0 C310,62.0 525.0,62.0 525.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20d1524cb969434fb478af6c31835105-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M310,184.0 L302,172.0 318,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20d1524cb969434fb478af6c31835105-0-2\" stroke-width=\"2px\" d=\"M430,182.0 C430,122.0 520.0,122.0 520.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20d1524cb969434fb478af6c31835105-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,184.0 L422,172.0 438,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-20d1524cb969434fb478af6c31835105-0-3\" stroke-width=\"2px\" d=\"M190,182.0 C190,2.0 530.0,2.0 530.0,182.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-20d1524cb969434fb478af6c31835105-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M530.0,184.0 L538.0,172.0 522.0,172.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Load the language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "sentence = \"John loves Mary.\"\n",
    "#sentence = 'Deemed universities charge huge fees'\n",
    "#sentence = 'I really,really like this guy.'\n",
    "#sentence = 'We have so much in common .'\n",
    "#sentence = 'When that started to  happen, a red flag went up in my head.'\n",
    "sentence = 'I have a red car.'\n",
    "\n",
    "# nlp function returns an object with individual token information, \n",
    "# linguistic features and relationships\n",
    "doc = nlp(sentence)\n",
    "\n",
    "print (\"{:<15} | {:<8} | {:<15} | {:<20}\".format('Token','Relation','Head', 'Children'))\n",
    "print (\"-\" * 70)\n",
    "\n",
    "for token in doc:\n",
    "    # Print the token, dependency nature, head and all dependents of the token\n",
    "    print (\"{:<15} | {:<8} | {:<15} | {:<20}\"\n",
    "         .format(str(token.text), str(token.dep_), str(token.head.text), str([child for child in token.children])))\n",
    "  \n",
    "# Use displayCy to visualize the dependency \n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 120})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STANZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 40.1MB/s]                    \n",
      "2021-10-21 12:43:03 INFO: Downloading default packages for language: en (English)...\n",
      "2021-10-21 12:43:04 INFO: File exists: /users/kent/slee122/stanza_resources/en/default.zip.\n",
      "2021-10-21 12:43:09 INFO: Finished downloading models and saved to /users/kent/slee122/stanza_resources.\n",
      "2021-10-21 12:43:09 WARNING: Can not find mwt: default from official model list. Ignoring it.\n",
      "2021-10-21 12:43:09 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2021-10-21 12:43:09 INFO: Use device: cpu\n",
      "2021-10-21 12:43:09 INFO: Loading: tokenize\n",
      "2021-10-21 12:43:09 INFO: Loading: pos\n",
      "2021-10-21 12:43:10 INFO: Loading: lemma\n",
      "2021-10-21 12:43:10 INFO: Loading: depparse\n",
      "2021-10-21 12:43:10 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('A', 2, 'det')\n",
      "('week', 5, 'obl:npmod')\n",
      "('after', 5, 'mark')\n",
      "('we', 5, 'nsubj')\n",
      "('met', 8, 'advcl')\n",
      "(',', 8, 'punct')\n",
      "('he', 8, 'nsubj')\n",
      "('texted', 0, 'root')\n",
      "('me', 8, 'obj')\n",
      "('and', 14, 'cc')\n",
      "('we', 14, 'nsubj')\n",
      "('did', 14, 'aux')\n",
      "(\"n't\", 14, 'advmod')\n",
      "('stop', 8, 'conj')\n",
      "('talking', 14, 'xcomp')\n",
      "('for', 19, 'case')\n",
      "('a', 19, 'det')\n",
      "('whole', 19, 'amod')\n",
      "('month', 15, 'obl')\n",
      "('and', 22, 'cc')\n",
      "('a', 22, 'det')\n",
      "('half', 19, 'conj')\n",
      "('.', 8, 'punct')\n",
      "Token           | Relation   | Head            \n",
      "--------------------------------------------------\n",
      "A               | det        | week            \n",
      "week            | obl:npmod  | met             \n",
      "after           | mark       | met             \n",
      "we              | nsubj      | met             \n",
      "met             | advcl      | texted          \n",
      ",               | punct      | texted          \n",
      "he              | nsubj      | texted          \n",
      "texted          | root       | ROOT            \n",
      "me              | obj        | texted          \n",
      "and             | cc         | stop            \n",
      "we              | nsubj      | stop            \n",
      "did             | aux        | stop            \n",
      "n't             | advmod     | stop            \n",
      "stop            | conj       | texted          \n",
      "talking         | xcomp      | stop            \n",
      "for             | case       | month           \n",
      "a               | det        | month           \n",
      "whole           | amod       | month           \n",
      "month           | obl        | talking         \n",
      "and             | cc         | half            \n",
      "a               | det        | half            \n",
      "half            | conj       | month           \n",
      ".               | punct      | texted          \n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download the language model\n",
    "stanza.download('en')\n",
    "\n",
    "sentence = 'A week after we met, he texted me and we didn\\'t stop talking for a whole month and a half.'\n",
    "#sentence = 'I have a red car.'\n",
    "\n",
    "# Build a Neural Pipeline\n",
    "nlp = stanza.Pipeline('en', processors = \"tokenize,mwt,pos,lemma,depparse\") \n",
    "\n",
    "# Pass the sentence through the pipeline\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Print the dependencies of the first sentence in the doc object\n",
    "# Format - (Token, Index of head, Nature of dependency)\n",
    "# Index starts from 1, 0 is reserved for ROOT\n",
    "doc.sentences[0].print_dependencies()\n",
    "\n",
    "\n",
    "print (\"{:<15} | {:<10} | {:<15} \".format('Token', 'Relation', 'Head'))\n",
    "print (\"-\" * 50)\n",
    "\n",
    "# Convert sentence object to dictionary  \n",
    "sent_dict = doc.sentences[0].to_dict()\n",
    "\n",
    "# iterate to print the token, relation and head\n",
    "for word in sent_dict:\n",
    "    print (\"{:<15} | {:<10} | {:<15} \"\n",
    "         .format(str(word['text']),str(word['deprel']), str(sent_dict[word['head']-1]['text'] if word['head'] > 0 else 'ROOT')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
