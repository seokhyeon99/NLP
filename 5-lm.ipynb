{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram Languag Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def preprocessSent(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "def preprocessText(text, tag = True):\n",
    "# Segment text into sentences\n",
    "    sent = sent_tokenize(text)\n",
    "# Tokenize each sentences\n",
    "    sent = [nltk.word_tokenize(s) for s in sent]\n",
    "# Part-of-speech tagging each sentences\n",
    "    if(tag == True) : sent = [nltk.pos_tag(s) for s in sent]\n",
    "    return sent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load NLTK books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "# import nltk resources\n",
    "import nltk\n",
    "from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'Moby',\n",
       "  'Dick',\n",
       "  'by',\n",
       "  'Herman',\n",
       "  'Melville',\n",
       "  '1851',\n",
       "  ']',\n",
       "  'ETYMOLOGY',\n",
       "  '.'],\n",
       " ['(',\n",
       "  'Supplied',\n",
       "  'by',\n",
       "  'a',\n",
       "  'Late',\n",
       "  'Consumptive',\n",
       "  'Usher',\n",
       "  'to',\n",
       "  'a',\n",
       "  'Grammar',\n",
       "  'School',\n",
       "  ')',\n",
       "  'The',\n",
       "  'pale',\n",
       "  'Usher',\n",
       "  '--',\n",
       "  'threadbare',\n",
       "  'in',\n",
       "  'coat',\n",
       "  ',',\n",
       "  'heart',\n",
       "  ',',\n",
       "  'body',\n",
       "  ',',\n",
       "  'and',\n",
       "  'brain',\n",
       "  ';',\n",
       "  'I',\n",
       "  'see',\n",
       "  'him',\n",
       "  'now',\n",
       "  '.']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## convert text1 into sentence segmented, tokenized list of lists \n",
    "text1_sent = preprocessText(\" \".join(text1), tag=False) # no tagging\n",
    "text1_sent[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram LM for sentence $S: a b c d ... $ \n",
    "\n",
    "Probability: \n",
    "- $p(d | abc ) = \\frac{\\mbox{count}(abcd)} { \\mbox{count}(abc)}$\n",
    "\n",
    "Self-Information:\n",
    "- $I(d | abc ) = log_2(\\frac{\\mbox{count}(abc)}{\\mbox{count}(abcd)})$\n",
    "\n",
    "Entropy of $S$:\n",
    "- $H(S) = 1/n \\sum_{abcd \\in S} I(d | abc)$\n",
    "\n",
    "Perplexity of $S$:\n",
    "- $PP(S) = 2^{H(S)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#############################################\n",
    "# compute n-grams dictionary for source text\n",
    "\n",
    "N = 2        # length of n-gram\n",
    "\n",
    "# data: list of sentences \n",
    "def nGramCount(data, N=2):\n",
    "    gramsC = {}  # dictionary to store n-gram counts\n",
    "    for seg in data:\n",
    "        \n",
    "        itm = seg.copy()\n",
    "            \n",
    "        # insert N-1 sentence starting and a sentence ending symbols\n",
    "        for i in range(N-1) :\n",
    "            itm.insert(0, \"///\")\n",
    "        itm.append(\"///\")\n",
    "\n",
    "        # Dictionary:  == count(a) -> count(ab)  \n",
    "        for i in range(len(itm) - N + 1):\n",
    "            # produce N-1 gram (a)\n",
    "            a  = ' '.join(itm[i:i+N-1]).lower()\n",
    "            # produce ngram (ab)\n",
    "            ab = ' '.join(itm[i:i+N]).lower()\n",
    "            # initialize dictionary\n",
    "            gramsC.setdefault(a, {})\n",
    "            gramsC[a].setdefault(ab, 0)\n",
    "            # count the ngram\n",
    "            gramsC[a][ab] += 1\n",
    "            \n",
    "    return(gramsC)\n",
    "\n",
    "\n",
    "# compute information of dictionary n-grams \n",
    "# I(b | a ) == log2(count(a) / count(ab))\n",
    "def nGramInfo(gramsC) :\n",
    "    nGrams = {}  # final n-gram dictionary with log-prob entries\n",
    "    nMin = 0\n",
    "    for a in gramsC:\n",
    "        # v : number of a n-grams \n",
    "        v = float(sum(gramsC[a].values()))\n",
    "        for ab in gramsC[a]:\n",
    "            \n",
    "            # Information: np.log2(v/gramG[b][ab]) == - np.log2(gramG[b][ab]/v)\n",
    "            nGrams[ab] = np.log2(v/gramsC[a][ab])\n",
    "            \n",
    "            if(nGrams[ab] > nMin): nMin = nGrams[ab]\n",
    "\n",
    "    # add a out-of-vocabulary probability\n",
    "    nGrams[\"|||OOV|||\"] = nMin+1\n",
    "    return(nGrams)\n",
    "\n",
    "\n",
    "# return list self-information per words \n",
    "def wordInfo(seg, nGrams, N=2, verbose = False):\n",
    "    inf = [] # list of word information value\n",
    "    itm = seg.copy()\n",
    "        \n",
    "    #adjust starting symbol for different length of n-grams N\n",
    "    for i in range(N-1): \n",
    "        itm.insert(0, \"///\")        \n",
    "    itm.append(\"///\")\n",
    "\n",
    "    # loop over all words for all n-grams\n",
    "    for i in range(len(itm) - N + 1):\n",
    "        \n",
    "        # compute n-gram ab\n",
    "        ab = ' '.join(itm[i:i+N]).lower()\n",
    "        \n",
    "        # check whether n-gram is in LM and get self information I(b | a)\n",
    "        try: \n",
    "            if(verbose) : print(\"nGram: {:10}\\t{:4.4}\".format(ab, nGrams[ab]))\n",
    "            inf.append(nGrams[ab])\n",
    "        except:\n",
    "            if(verbose) : print(\"nGOOV: {:10}\\t{:4.4}\".format(ab, nGrams[\"|||OOV|||\"]))\n",
    "            inf.append(nGrams[\"|||OOV|||\"])\n",
    "    return(inf)\n",
    "\n",
    "# compute perplexity from list of word information\n",
    "def perplexity(info):\n",
    "    return(2**(sum(info) / float(len(info))))\n",
    "\n",
    "# count number of occurances of n-gram g in LM \n",
    "def countOcc(g, LM):\n",
    "    return (sum(LM[g].values()) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: f('pains' | *):   \t 15\n",
      "count: f('more'  | *):   \t 508\n",
      "count: f('pains' | 'more'):\t 1\n",
      "info:  I('pains' | 'more'):\t 8.988684686772165\n",
      "\n",
      "count: f('much' | *):    \t 223\n",
      "count: f('more' | 'much'):\t 14\n",
      "info:  I('more' | 'much'):\t 3.9935449778627006\n"
     ]
    }
   ],
   "source": [
    "## make a 2-gram languag model\n",
    "# store n-grams counts in dictionary \n",
    "\n",
    "text1_count2 = nGramCount(text1_sent)\n",
    "\n",
    "# compute information values from counts\n",
    "text1_info2 = nGramInfo(text1_count2)\n",
    "\n",
    "\n",
    "print(\"count: f('pains' | *):   \\t\", countOcc('pains',text1_count2))\n",
    "print(\"count: f('more'  | *):   \\t\", countOcc('more', text1_count2))\n",
    "print(\"count: f('pains' | 'more'):\\t\", text1_count2['more']['more pains'])\n",
    "print(\"info:  I('pains' | 'more'):\\t\", text1_info2['more pains'])\n",
    "\n",
    "print(\"\\ncount: f('much' | *):    \\t\", countOcc('much', text1_count2))\n",
    "print(\"count: f('more' | 'much'):\\t\", text1_count2['much']['much more'])\n",
    "print(\"info:  I('more' | 'much'):\\t\", text1_info2['much more'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: f('much the' | *):     \t 20\n",
      "count: f('more' | 'much the'):\t 9\n",
      "count: f('speed'| 'much the'):\t 1\n",
      "\n",
      "info:  I('more' | 'much the'):\t 1.15200309344505\n",
      "info:  I('speed'| 'much the'):\t 4.321928094887363\n"
     ]
    }
   ],
   "source": [
    "## make an 3-gram information dictionary p(c | ab)\n",
    "\n",
    "text1_count3 = nGramCount(text1_sent, N=3)\n",
    "text1_info3 = nGramInfo(text1_count3)\n",
    "\n",
    "print(\"count: f('much the' | *):     \\t\", countOcc('much the', text1_count3))\n",
    "print(\"count: f('more' | 'much the'):\\t\", text1_count3['much the']['much the more'])\n",
    "print(\"count: f('speed'| 'much the'):\\t\", text1_count3['much the']['much the speed'])\n",
    "\n",
    "print(\"\\ninfo:  I('more' | 'much the'):\\t\", text1_info3['much the more'])\n",
    "print(\"info:  I('speed'| 'much the'):\\t\", text1_info3['much the speed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: f('much the more' | *):  \t 9\n",
      "count: f('a' | 'much the more'):\t 2\n",
      "count: f('pains' | 'much the more'):\t 1\n",
      "\n",
      "\n",
      "info:  I('a' | 'much the more'):\t 2.169925001442312\n",
      "info:  I('pains' | 'much the more'):\t 3.169925001442312\n"
     ]
    }
   ],
   "source": [
    "## make an 4-gram information dictionary p(d | abc)\n",
    "\n",
    "text1_count4 = nGramCount(text1_sent, N=4)\n",
    "text1_info4 = nGramInfo(text1_count4)\n",
    "\n",
    "print(\"count: f('much the more' | *):  \\t\", countOcc('much the more', text1_count4))\n",
    "print(\"count: f('a' | 'much the more'):\\t\", text1_count4['much the more']['much the more a'])\n",
    "print(\"count: f('pains' | 'much the more'):\\t\", text1_count4['much the more']['much the more pains'])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"info:  I('a' | 'much the more'):\\t\",  text1_info4['much the more a'])\n",
    "print(\"info:  I('pains' | 'much the more'):\\t\", text1_info4['much the more pains'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51.693482447884485"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# perplexity per sentence \n",
    "sent1_info2 = wordInfo(text1_sent[1], text1_info2, N=2)\n",
    "perplexity(sent1_info2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.165996948635516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_info3 = wordInfo(text1_sent[1], text1_info3, N=3)\n",
    "perplexity(sent1_info3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.541746051768873"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1_info4 = wordInfo(text1_sent[1], text1_info4, N=4)\n",
    "perplexity(sent1_info4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Michelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "michelle = \"\"\"Hi! I'm Michelle and I'm 22.\n",
    "I really,really like this guy. He's 27 and everything  I like in a guy. We have so much in common.\n",
    "We met around three and a half months ago. A week after we met, he texted me and we didn't stop talking for a whole month and a half. We talked day and night, sometimes 'til four in the morning.\n",
    "Then, he started ignoring me. When that started to  happen, a red flag went up in my head, so I started ignoring him, too. Except I started missing him.\n",
    "Before I started a new semester, I asked him what was the point of saving my number if he wasn't going to ask me out. (Yes, we haven't gone out on a date yet. We've talked about it, but he doesn't make it happen.)\n",
    "I told him I wasn't going to have enough time for him, and if he really wanted to go out with me, he should make it happen soon rather than later.\n",
    "I just don't understand why he hasn't asked me out yet. He gives me the money excuse, or the \"every time I want to, something else comes up\" excuse.\n",
    "If he wants to see me he should've done so already... right?\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessText' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c301ca41a8cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# pre-process (sentence/tokenize) michelle text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmichelle_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmichelle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Michelle[1]:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmichelle_sent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessText' is not defined"
     ]
    }
   ],
   "source": [
    "# compute perplexity of michelle sentence with non-stemmed LM\n",
    "\n",
    "# pre-process (sentence/tokenize) michelle text\n",
    "michelle_sent = preprocessText(michelle, tag=False)\n",
    "print(\"Michelle[1]:\\t\", michelle_sent[1], \"\\n\")\n",
    "\n",
    "# compute n-gram probabilities for words\n",
    "michelle_info = wordInfo(michelle_sent[1], text1_info2, verbose=True)\n",
    "\n",
    "print(\"\\nInformation:\\t\", michelle_info, \"\\n\\nperplexity:\", perplexity(michelle_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'mobi', 'dick', 'by', 'herman', 'melvil', '1851', ']', 'etymolog', '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "import re\n",
    "\n",
    "# instantiate stemmers \n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Tokenizing for porter:\n",
    "text1_porter = [[porter.stem(word) for word in sent] for sent in text1_sent]\n",
    "text1_porter[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unstemmed\n",
      "count: f('pains' | *):    \t 15\n",
      "count: f('pain' | *):    \t 2\n",
      "count: f('more' | *):    \t 508\n",
      "count: f('pains' | 'more'):\t 1\n",
      "probs: p('pains' | 'more'):\t 8.988684686772165\n",
      "\n",
      "porter\n",
      "count: f('pain' | *):    \t 25\n",
      "count: f('more' | *):    \t 508\n",
      "count: f('pain' | 'more'):\t 1\n",
      "count: f('pain' | 'more'):\t 8.988684686772165\n"
     ]
    }
   ],
   "source": [
    "# gnerate LM from stemmed text1\n",
    "# LM from txt1 tokenized by Porter:\n",
    "text1_porter_count2 = nGramCount(text1_porter)\n",
    "text1_porter_info2 = nGramInfo(text1_porter_count2)\n",
    "\n",
    "print(\"unstemmed\")\n",
    "print(\"count: f('pains' | *):    \\t\", countOcc('pains', text1_count2))\n",
    "print(\"count: f('pain' | *):    \\t\", countOcc('pain', text1_count2))\n",
    "print(\"count: f('more' | *):    \\t\", countOcc('more', text1_count2))\n",
    "print(\"count: f('pains' | 'more'):\\t\", text1_count2['more']['more pains'])\n",
    "print(\"probs: p('pains' | 'more'):\\t\", text1_info2['more pains'])\n",
    "\n",
    "print(\"\\nporter\")\n",
    "print(\"count: f('pain' | *):    \\t\", countOcc('pain', text1_porter_count2))\n",
    "print(\"count: f('more' | *):    \\t\", countOcc('more', text1_porter_count2))\n",
    "print(\"count: f('pain' | 'more'):\\t\", text1_porter_count2['more']['more pain'])\n",
    "print(\"count: f('pain' | 'more'):\\t\", text1_porter_info2['more pain'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity: 51.693482447884485 \tporter PP: 56.325385841015915\n"
     ]
    }
   ],
   "source": [
    "sent1_probs2        = wordInfo(text1_sent[1], text1_info2)\n",
    "sent1_porter_probs2 = wordInfo(text1_porter[1], text1_porter_info2)\n",
    "\n",
    "print(\"perplexity:\", perplexity(sent1_probs2), \n",
    "      \"\\tporter PP:\", perplexity(sent1_porter_probs2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michelle[1]:\t ['I', \"'m\", 'Michelle', 'and', 'I', \"'m\", '22', '.'] \n",
      "Porter[1]:\t ['I', \"'m\", 'michel', 'and', 'I', \"'m\", '22', '.']\n",
      "nGram: /// i     \t4.674\n",
      "nGOOV: i 'm      \t15.23\n",
      "nGOOV: 'm michel \t15.23\n",
      "nGOOV: michel and\t15.23\n",
      "nGram: and i     \t6.176\n",
      "nGOOV: i 'm      \t15.23\n",
      "nGOOV: 'm 22     \t15.23\n",
      "nGOOV: 22 .      \t15.23\n",
      "nGram: . ///     \t0.2617\n",
      "\n",
      "Information:\t [4.67401206866744, 15.23099611713346, 15.23099611713346, 15.23099611713346, 6.176439322543313, 15.23099611713346, 15.23099611713346, 15.23099611713346, 0.2617297369843345] \n",
      "\n",
      "Perplexity: 2681.2391764281415\n"
     ]
    }
   ],
   "source": [
    "# Michelle with porter stemmer\n",
    "michelle_porter = [[porter.stem(word) for word in sent] for sent in michelle_sent]\n",
    "print(\"Michelle[1]:\\t\", michelle_sent[1], \"\\nPorter[1]:\\t\", michelle_porter[1])\n",
    "\n",
    "michelle_porter_info = wordInfo(michelle_porter[1], text1_porter_info2, verbose=True)\n",
    "print(\"\\nInformation:\\t\", michelle_porter_info, \"\\n\\nPerplexity:\", perplexity(michelle_porter_info))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks \n",
    "prepare LM in different ways and test on michelle text:\n",
    "- use your stemmer \n",
    "- use NER\n",
    "- add more data (text2 ... text9) to LM \n",
    "- apply to michelle text\n",
    "\n",
    "compare perplexity scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from stemming import * #myStemmer2(wrd, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from NE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'michelle_sent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-46908e215154>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmichelle_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'michelle_sent' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(michelle_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
